{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvAqpjaxShg0UeopoVhUhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongug/Fly_python/blob/main/3_documents_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGd0WX0mpLjY",
        "outputId": "b9fb34a9-02e8-45e2-fbf3-dddba2a148b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.11)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.2.28)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.0)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.97)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=d4266c7d915bb2a73959ed516fb1b4e3ab550952012b96309d6c4dba4a315aa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-core\n",
        "!pip install pypdf docx2txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WebBaseLoader\n",
        "\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://www.naver.com\")\n",
        "data = loader.load()\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQRrxCvrpacX",
        "outputId": "d5f0712b-e168-4e59-b380-8c770c842c12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'https://www.naver.com', 'title': 'NAVER', 'description': 'ÎÑ§Ïù¥Î≤Ñ Î©îÏù∏ÏóêÏÑú Îã§ÏñëÌïú Ï†ïÎ≥¥ÏôÄ Ïú†Ïö©Ìïú Ïª®ÌÖêÏ∏†Î•º ÎßåÎÇò Î≥¥ÏÑ∏Ïöî', 'language': 'ko'}, page_content='        NAVER                             ÏÉÅÎã®ÏòÅÏó≠ Î∞îÎ°úÍ∞ÄÍ∏∞ ÏÑúÎπÑÏä§ Î©îÎâ¥ Î∞îÎ°úÍ∞ÄÍ∏∞ ÏÉàÏÜåÏãù Î∏îÎ°ù Î∞îÎ°úÍ∞ÄÍ∏∞ ÏáºÌïë Î∏îÎ°ù Î∞îÎ°úÍ∞ÄÍ∏∞ Í¥ÄÏã¨ÏÇ¨ Î∏îÎ°ù Î∞îÎ°úÍ∞ÄÍ∏∞ MY ÏòÅÏó≠ Î∞îÎ°úÍ∞ÄÍ∏∞ ÏúÑÏ†Ø Î≥¥Îìú Î∞îÎ°úÍ∞ÄÍ∏∞ Î≥¥Í∏∞ ÏÑ§Ï†ï Î∞îÎ°úÍ∞ÄÍ∏∞               Í≤ÄÏÉâ                       Í≤ÄÏÉâ       ÏûÖÎ†•ÎèÑÍµ¨     ÏûêÎèôÏôÑÏÑ±/ÏµúÍ∑ºÍ≤ÄÏÉâÏñ¥ÌéºÏπòÍ∏∞                          ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prdFx59mqmFf",
        "outputId": "1559b890-7666-4042-fab8-bb26f84c664f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        NAVER                             ÏÉÅÎã®ÏòÅÏó≠ Î∞îÎ°úÍ∞ÄÍ∏∞ ÏÑúÎπÑÏä§ Î©îÎâ¥ Î∞îÎ°úÍ∞ÄÍ∏∞ ÏÉàÏÜåÏãù Î∏îÎ°ù Î∞îÎ°úÍ∞ÄÍ∏∞ ÏáºÌïë Î∏îÎ°ù Î∞îÎ°úÍ∞ÄÍ∏∞ Í¥ÄÏã¨ÏÇ¨ Î∏îÎ°ù Î∞îÎ°úÍ∞ÄÍ∏∞ MY ÏòÅÏó≠ Î∞îÎ°úÍ∞ÄÍ∏∞ ÏúÑÏ†Ø Î≥¥Îìú Î∞îÎ°úÍ∞ÄÍ∏∞ Î≥¥Í∏∞ ÏÑ§Ï†ï Î∞îÎ°úÍ∞ÄÍ∏∞               Í≤ÄÏÉâ                       Í≤ÄÏÉâ       ÏûÖÎ†•ÎèÑÍµ¨     ÏûêÎèôÏôÑÏÑ±/ÏµúÍ∑ºÍ≤ÄÏÉâÏñ¥ÌéºÏπòÍ∏∞                          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/DJI_Osmo_Pocket_3_User_Manual_v1.0_en.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "print(pages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt1uvWcLreVj",
        "outputId": "79e33d0d-284b-43cb-b8a3-a7c498d3b4ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='2023.10 v1.0User Manual' metadata={'source': '/content/DJI_Osmo_Pocket_3_User_Manual_v1.0_en.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyU4BdeAsxTq",
        "outputId": "957f2cfd-2143-4469-f4a1-d85363aeaa2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 © 0000 大疆创新 版权所有 Searching for Keywords\n",
            "Search for keywords such as “battery” and “install” to find a topic. If you are using Adobe \n",
            "Acrobat Reader to read this document, press Ctrl+F on Windows or Command+F on Mac \n",
            "to begin a search.\n",
            " Navigating to a Topic\n",
            "View a complete list of topics in the table of contents. Click on a topic to navigate to that \n",
            "section.\n",
            " Printing this Document\n",
            "This document supports high resolution printing.This document is copyrighted by DJI with all rights reserved. Unless otherwise authorized by \n",
            "DJI, you are not eligible to use or allow others to use the document or any part of the document \n",
            "by reproducing, transferring or selling the document. Users should only refer to this document \n",
            "and the content thereof as instructions to operate DJI UAV. The document should not be used \n",
            "for other purposes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import Docs2txtLoader\n",
        "loader = Docs2txtLoader(\"/content/LLM이 만들어 내는 혁명과 교통산업.docx\")\n",
        "pages = loader.load()\n",
        "\n",
        "print(pages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "LyRzxkLC_V_F",
        "outputId": "5b993269-fbcb-41e6-8c06-c9c4cc9ba436"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Docs2txtLoader' from 'langchain.document_loaders' (/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-31af7ae29e0d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocs2txtLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocs2txtLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/LLM이 만들어 내는 혁명과 교통산업.docx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Docs2txtLoader' from 'langchain.document_loaders' (/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "# Docs2txtLoader is deprecated and replaced with Docx2txtLoader\n",
        "\n",
        "loader = Docx2txtLoader(\"/content/LLM이 만들어 내는 혁명과 교통산업.docx\")\n",
        "pages = loader.load()\n",
        "\n",
        "print(pages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WWJNc4EAiwT",
        "outputId": "bc4e2a38-b3fe-4948-dad2-7bfa8f5551d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n",
            "page_content='LLM이 만들어 내는 혁명과 교통산업\n",
            "\n",
            "김영욱\n",
            "\n",
            "Microsoft에서 플랫폼 사업부에서 수석으로 일했으며 지금은 AI 전문기업인 Hello AI 창업해서 인공지능과 관련한 프로젝트 및 과제를 수행하고 있으며 강연과 집필 그리고 교육에도 힘쓰고 있다. \n",
            "\n",
            "\n",
            "\n",
            " 최근 모든 분야에서 이전에 없었던 급격한 기술 발전이 사회의 모습을 바꾸고 또 바뀐 사회 필요를 또 다시 새로운 기술로 채워 나가는 빠른 순환이 일어 나고 있다. 그런데 변화를 주도하는 기술 중에서 특히 IT 기술은 더더욱 그런 현상을 가속화 시킬 뿐만 아니라 주도하고 있는 현상을 우리는 쉽게 볼 수 있다. \n",
            "\n",
            "코로나 이후 전 세계적으로 어려운 경제 상황을 보여주고 있지만 상대적으로 유럽은 더더욱 큰 경제 불황의 고리에 빠져들고 있는데 유럽 경제가 힘든 이유는 우크라이나 전쟁을 비롯, 에너지 문제를 포함해서 여러 이유를 함께 생각해 볼 수 있지만 IT산업이 제대로 발전하지 못한 이유도 크게 한 몫하고 있다. 우리가 주변에서 편하게 사용하는 다양한 인터넷 서비스들 중 유럽 것을 찾아보기 힘든 것만 봐도 유럽은 여전히 전통적인 산업에 머물러 있다는 것을 알 수 있다. 또 미국의 S&P 500의 시가 총액 상위 종목들을 보면 애플, Microsoft, 구글, 아마존 그리고 테슬라 등 IT 관련 기술주가 상위권을 휩쓸고 있는 것을 볼 수 있다. 그렇게 우리는 IT 기술이 곧 경쟁력인 세상의 한복판에 살아가고 있다. \n",
            "\n",
            "\n",
            "\n",
            "The age of Generative AI \n",
            "\n",
            " 작년 말 갑자기 등장한 ChatGPT는 최근 모든 이슈를 가져와도 비교가 되지 않을 만큼 큰 이슈를 일으키고 있다. 사람들은 ChatGPT를 보자마자 IT 역사에 큰 변화를 일으킬 또 하나의 마일스톤이 시작되었다는 생각 보다는 각자 필요한 용도로 활용 할 수 있다는 점에 열광하기 시작했다. 100만 사용자가 단 5일만에 가입한 사실만 보더라도 사람들이 얼마나 ChatGPT에 열광하고 있는 것을 알 수 있다.  \n",
            "\n",
            " <그림1> 주요 서비스의 회원 100만명 달성 시간 (출처: statista)\n",
            "\n",
            " ChatGPT 이전에 세상에서 이슈 몰이를 했던 인공지능으로는 구글의 알파고를 빼놓고 이야기 할 수 없다. 알파고는 바둑을 혹은 바둑만 전문적으로 추론 할 수 있는 인공지능이었다. 최근 알파고 이야기를 주변에서 찾아 볼 수 없는 이유는 알파고를 바둑 이외의 영역에 적용했을 때 인상적인 결과를 만들지 못했기 때문일지도 모른다. 항상 이런 전문성이라는 것은 양날의 검과 같아서 장점과 치명적인 한계를 함께 동반하는 것이 일반적이다. 비슷하게 우리 주변의 인공지능 시스템들은 대부분은 한정된 영역의 일만 할 수 있는 소위 말하는 전문가 시스템이었다. 집안의 온도를 맞추어 주고 공장의 품질 관리를 해주고 농장의 작물들을 키워주는 인공지능 시스템들도 모두 특정 용도에 최적화된 전문가 시스템이라고 할 수 있다. \n",
            "\n",
            "하지만 사람들이 인공지능에 거는 기대치는 이보다 훨씬 높다. 우리가 아이언맨 등의 영화에서 봤던 자비스와 같은 인공지능은 어떨까? 주인공인 토니 스타크가 묻는 질문에 적절하게 대답해주고 주변에 있는 시설들을 제어하면서 주인공의 또 하나의 동료로 완전히 함께 일하는 모습을 볼 수 있다. 사실 자비스와 같이 다양한 용도로 사용 할 수 있는 수준의 인공지능을 인공 일반 지능(AGI: Artificial General Intelligence) 기술이라고 말한다. 인공 일반 지능은 전문가 시스템에 반대되는 개념으로 사람들의 말과 말속의 맥락을 정확하게 이해하고 거기에 맞게 동작하거나 답을 해주는 것이 목표이다. ChatGPT는 이제까지 나왔던 그 어떤 인공 지능 기술 보다도 인공 일반 지능에 가장 가까운 기술이었고 그래서 사람들은 ChatGPT를 보면서 자비스를 떠올리고 있었는지도 모르겠다. \n",
            "\n",
            "\n",
            "\n",
            "전 세계의 말들을 학습한 LLM\n",
            "\n",
            "이전에는 컴퓨터에게 사람의 말을 가르치기 위해서 문법적인 접근을 했다. 언어들의 문법적인 원리를 연구하고 발견된 원리들을 컴퓨터에 소프트웨어로 제작하는 방식으로 접근을 했지만 의미 있는 결과를 도출하는 것이 어려웠다. 문법적인 접근이 어려움을 겪었던 이유는 모든 사람들이 문법적으로 이야기 하지 않는다는 점이 첫 번째 문제였다. 문법이라는 것이 일정한 듯 하면서도 모든 언어적인 현상들을 모두 문법으로 설명할 수 없었다. 또 다른 이유는 사람들은 계속해서 새로운 단어들을 만들어 내고 있다는 점이다. 단적인 예를 들자면 ‘구글링’이라는 단어는 구글이 등장하기 전에는 존재하지 않았던 동사였다. 그래서 문법적인 접근으로는 한계가 있음을 인정하지 않을 수 없었다. 문법적인 접근을 하는 대신 사람들이 실제 사용하는 말을 긁어 모아서 단어 사이의 가중치를 매기는 방식으로 접근하기 시작했다. 이 방식의 경우 사람들의 말의 양을 늘리면 늘릴 수록 결과물도 함께 좋아지는 장점이 있었고 또 인터넷 등에서 사람들이 실제로 사용하는 말을 모아서 학습하다 보니 새로운 단어들과 기존에 알려진 문법과 상관없는 현상들을 모두 분석할 수 있는 장점이 있었다. 문제는 말의 양을 늘리면 늘릴 수록 분석하는데 들어가는 컴퓨팅 비용도 기하급수적으로 늘어난다는 점에 있었다. 그래서 무한정으로 학습량을 늘리는 것은 어려웠다. 어찌되었든 이렇게 사람의 말을 학습해 놓은 것을 언어 모델(LM: Language Model)이라 부르는데 이 중에서도 특히 언어 모델을 엄청난 크기로 만들어 놓은 것을 거대 언어 모델(LLM: Large Language Model)이라고 불렀다. \n",
            "\n",
            "\n",
            "\n",
            "데이터세트\n",
            "\n",
            "토큰\n",
            "\n",
            "교육 비율\n",
            "\n",
            "일반 크롤링\n",
            "\n",
            "4100억개\n",
            "\n",
            "60%\n",
            "\n",
            "웹텍스트 2\n",
            "\n",
            "190억개\n",
            "\n",
            "22%\n",
            "\n",
            "책1\n",
            "\n",
            "120억개\n",
            "\n",
            "8%\n",
            "\n",
            "책2\n",
            "\n",
            "550억개\n",
            "\n",
            "8%\n",
            "\n",
            "위키백과\n",
            "\n",
            "30억개\n",
            "\n",
            "3%\n",
            "\n",
            " <표1> 대표적인 LLM인 GPT3의 학습에 사용된 데이터 양\n",
            "\n",
            "거대 언어 모델을 만드는 데에는 엄청난 비용과 컴퓨팅 인프라가 필요한 까닭에 주요 소프트웨어 회사에서 개발하고 있다. 최근에는 주로 구글, Microsoft, 엔비디아, 그리고 오픈AI와 같은 글로벌 대형 업체들이 주도적으로 연구와 개발을 하고 있는데 이중에서도 오픈AI는 상대적으로 작은 규모의 회사이긴 하지만 Microsoft가 14조가 넘는 비용을 투자하면서 선두로 나서게 되었으며 언어 모델인 GPT, 이미지를 생성하는 DALL-E2 이외에도 많은 연구를 하고 있다. \n",
            "\n",
            "\n",
            "\n",
            "지식전이 현상의 발견 \n",
            "\n",
            "거대 언어 모델을 만드는 과정에서 수 많은 말들을 처리하다 보니 잘 만들어진 언어 모델은 그 자체로 많은 잠재성을 가질 수 있게 되었다. 언어 모델에 들어 있는 말들 자체가 하나의 거대한 지식 체계로 동작하면서 새로운 능력들이 발현하게 되었는데 이와 같은 현상을 지식의 전이(Knowledge Transfer)라 부르게 되었다. 실제로 오픈AI가 만든 GPT-3 모델의 경우 기본적으로 감정의 분석, 문서의 요약, 번역, 질의응답, 작문, 문장의 재구성 등 많은 능력을 가지고 있어서 이미 많은 분야에서 이 능력을 활용하고 있거나 활용하기 위해서 시도하고 있다. GPT-4의 경우 이 보다 훨씬 더 큰 잠재력이 있는 것으로 알려져 있다. 지식의 전이 현상으로 인해 다양한 일들을 척척 해내는 것을 보면 거대 언어 모델의 능력이 무한 할 것 같지만 ChatGPT 역시 만능은 아니다. ChatGPT가 나름 다양한 능력을 가지고 있고 이전에 없었던 새로운 일들을 가능하게 해주는 놀라운 존재이긴 하지만 한계도 역시 분명하다. ChatGPT는 다음과 같이 알려진 한계가 있다. \n",
            "\n",
            "\n",
            "\n",
            "2021년 9월에 멈춘 학습\n",
            "\n",
            "일관성 없는 결과물\n",
            "\n",
            "거짓 정보 (Hallucination)\n",
            "\n",
            "영어에 비해 떨어지는 한국어 결과물\n",
            "\n",
            "\n",
            "\n",
            "ChatGPT를 학습 시킬 때 전세계의 인터넷과 책에 있는 자료들을 긁어 모아서 학습을 시켰지만 그 학습 자료는 2021년 9월에 머물러 있다. 따라서 최신 정보를 묻는다면 대답을 할 수 없다. 가령 예를 들자면 \n",
            "\n",
            "\n",
            "\n",
            "‘지금 한국의 대통령은?’, \n",
            "\n",
            "‘오늘 삼성전자 주가는 얼마야?’,\n",
            "\n",
            "’내일 비가 올 것 같아?’\n",
            "\n",
            "\n",
            "\n",
            "이런 질문들에 대해서는 답변을 할 수 없다. 또 ChatGPT 자체는 학습된 데이터를 기반으로 말을 생성해 내는 것이 주된 기능이고 생성된 말의 일관성이나 혹은 사실 여부를 검증해 주지는 않는다. 그래서 질문 할 때 마다 다른 말을 만들어 내주는가 하면 또 때로는 아주 아주 진지하고 그럴싸한 거짓말을 하기도 한다. 모르는 사람들은 ChatGPT가 만들어내는 거짓말을 그대로 믿을 만큼 거짓말을 잘한다. 또 다른 문제로는 GPT-4에 이르러서는 조금 나아지긴 했지만 기본적으로 한국어 보다는 영어의 학습량이 많기 때문에 한국어 보다는 영어가 더 좋은 결과물을 보여주기도 한다. \n",
            "\n",
            "\n",
            "\n",
            "생성형 인공지능의 시대가 왔다. \n",
            "\n",
            "위에서 ChatGPT의 한계를 이야기 했지만 업계에서는 빠르게 한계를 극복하면서 빠르게 생성형 인공지능을 제품이나 서비스로 연결하고 있다. 생성형 인공지능과 관련해서는 구글, 메타, 엔비디아 그리고 국내의 네이버 등도 빠르게 움직이고 결과물을 내놓고 있지만 최근 이 분야에서 가장 빠르게 움직이고 있는 기업은 바로 Microsoft이다. \n",
            "\n",
            " Microsoft는 검색엔진인 빙(Bing)에 생성형 AI 기술을 적용해서 대화형으로 문제를 해결할 수 있는 방법을 제공하면서 빠르게 구글의 점유율을 가져오고 있다. 또 빙 이미지 크리에이터를 통해서 사용자들에게 필요한 이미지를 생성 할 수 있는 방법도 함께 제공하고 있다. \n",
            "\n",
            " Microsoft의 전략에서 특히 눈의 띄는 부분은 바로 플랫폼이다. Microsoft는 거대 언어 모델을 그 자체로 사용하지 않고 개발을 위한 시멘틱 커널 프레임웍(Semantic Kernel Framework)을 먼저 만들고 그 기반으로 코파일럿(Copilot)라는 이름으로 플랫폼화를 했다. 플랫폼화가 잘되고 나면 안정적으로 새로운 많은 제품과 서비스에서 거대 언어 모델의 효과를 즉시 발휘할 수 있게 확장 할 수 있게 된다. \n",
            "\n",
            " Microsoft는 Copilot 서비스를 소프트웨어 개발 도구로 활용할 수 있게 Github Copilot으로 출시해서 소프트웨어 개발 분야에서 활용 하는가 하면 일반인들을 위해서 윈도우 11에서 거대 언어 모델을 사용할 수 있는 Windows Copilot의 오픈 베타 버전을 내놓았다. \n",
            "\n",
            "\n",
            "<그림2> Windows Copilot (출처: Windows 11 Copilot 테스트 받은 편지함, ChatGPT 및 Bing AI 플러그인 통합 - GAMINGDEPUTY KOREA)\n",
            "\n",
            " 뿐만 아니라 워드, 엑셀, 파워포인트, 팀즈, 아웃룩 등 오피스 제품들을 클라우드 기반으로 제공하는 Microsoft 365에도 역시 코파일럿 제품이 들어가면서 이제 부터는 문서 작성이나 엑셀에서 데이터 분석, 발표용 슬라이드 작성등 대부분의 작업을 이제부터는 코파일럿과의 채팅을 통해서 해결할 수 있는 시대가 왔다. \n",
            "\n",
            " Microsoft 뿐만 아니다. 포토샵으로 유명한 어도비(Adobe)도 적극적으로 생성형 인공지능을 활용하고 있다. 포토샵에서 여러 도구들을 사용해서 사람이 이미지를 일일이 수정하는 것이 아니라 생성형 인공지능을 사용해서 영역을 지정하고 어떻게 수정할 것인지 말로 표현하면 인공지능이 이미지를 수정해주는 식으로 동작 할 수 있다. \n",
            "\n",
            "\n",
            "\n",
            "<그림3> 포토샵에서 생성형 인공지능의 활용(Photoshop Has Become a Lot More Powerful Thanks to Brand New Generative AI Features Using Adobe's Firefly (wccftech.com)\n",
            "\n",
            "이렇게 생성형 인공지능 기술들은 IT 산업 전반에서 IT와 사람 사이의 인터페이스를 변화 시키고 있다. 버튼을 누르거나 사용자의 입력에 의해서 반응하던 시스템이 아니라 사람과의 대화를 통해서 내용을 이용하고 함께 작업에 동참하는 자비스가 어느덧 이제는 우리 곁으로 들어왔다. \n",
            "\n",
            "\n",
            "\n",
            "교통산업과 거대 언어 모델 \n",
            "\n",
            "미국 최대 자동차 회사인 제너럴모터스(GM)이 ChatGPT를 자사 차량에 적용하는 방안을 모색하고 있다는 발표를 했다. 이런 변화가 자연스럽게 느껴지는 것은 자동차라는 공간이 단순한 이동 수단을 넘어서서 이제는 하나의 스마트 디바이스로 진화하고 있기 때문이다. 고성능 컴퓨터가 탑재되어서 완벽하지는 않아도 운전 보조 장치로는 충분한 수준의 자율주행을 수행할 수 있게 되었고 음성으로 대화가 가능한 인터페이스가 탑재되기 시작했다. \n",
            "\n",
            " 자동차로 이동할 때 필요한 주요 장치들의 제어 뿐만 아니라 네비게이션을 포함한 인포테인먼트 그리고 오디오와 함께 다양한 콘텐츠를 소비할 수 있게 발전하고 있는 엔터테인먼트까지 생성형 인공지능을 활용할 수 있는 분야는 무궁 무진하다. 이것은 단순한 가능성이 아니라 처음에는 프리미엄급의 기준이 생성형 인공지능의 탑재 여부가 될 예정이고 더 나아가 전체적인 자동차의 기능과 어떻게 잘 통합할 수 있는지 여부는 자동차 회사의 미래 가치에 치명적인 영향을 끼치게 될 것이다. \n",
            "\n",
            " 이런 면에 있어서 이미 충분한 컴퓨팅 파워를 탑재하고 있는 프리미엄급의 전기 자동차들은 생성형 인공지능의 초기 데뷔 무대가 될 확률이 높다. 지금까지 전기 자동차는 베터리와 모터와 같은 전기 장비들을 잘 통합하고 경쟁력 있는 가격에 제공하는 것이 주요 경쟁력이었다면 앞으로는 전기 계통의 플랫폼들은 모두 기본 이상의 품질을 갖추게 되고 결국 사람들이 직접 느낄 수 있는 편의성과 고급스러움이 경쟁력이 될 예정이며 여기에서 핵심 기술이 ChatGPT와 같은 LLM 기술이 될 것으로 보인다. \n",
            "\n",
            "\n",
            "\n",
            "<그림4> 자동차 산업에서의 생성형 인공지능 (출처: AI In Cars: 14 Types of Automotive AI | Built In)\n",
            "\n",
            "\n",
            "\n",
            " 자동차만 변화의 대상이 되지 않는다. 교통이란 항상 한정된 자원인 도로를 이용해서 원활한 이동을 이끌어 내기 위해서 항상 많은 데이터를 거의 실시간으로 처리하는 것이 일상이다. 이런 데이터를 기반으로 의사결정을 하고 차량의 흐름을 통제하는데 역시 생성형 인공지능이 적극적으로 활용 될 수 있다. 이미 지금도 데이터에 대해서 질문만 했을 뿐인데 그 답을 찾기 위해서 생성형 인공지능이 스스로 데이터를 분석하기 위한 코드를 작성하고 실행해서 얻어진 결과로 답변을 작성해서 알려준다. \n",
            "\n",
            " ChatGPT의 이런 발전 속도와 변화는 사실 IT업계에 종사하는 사람들 조차 받아들이기 버거울 정도로 크다. 하지만 1년이 되지 않는 이런 변화의 시작이 3년이 지났을 때 그리고 10년이 지났을 때 또 얼마나 큰 변화를 가져다 줄지는 가늠하기 어렵다. 하지만 한 가지 확실한 것은 그 변화에 전면부에 올라타는 것과 그렇지 못한 경우의 차이는 치명적일 것이라는 것 만큼은 분명할 것이다.' metadata={'source': '/content/LLM이 만들어 내는 혁명과 교통산업.docx'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "loader = CSVLoader('/content/titanic3.csv',\n",
        "                   csv_args={\n",
        "                       'delimiter': ',',\n",
        "                       'quotechar': '\"',\n",
        "                       'fieldnames': ['Pclass',\n",
        "                                      'Survived',\n",
        "                                      'Name',\n",
        "                                      'Sex',\n",
        "                                      'Age',\n",
        "                                      'SibSp',\n",
        "                                      'Parch',\n",
        "                                      'Ticket',\n",
        "                                      'Fare',\n",
        "                                      'Cabin',\n",
        "                                      'Embarked',\n",
        "                                      'Boat',\n",
        "                                      'Body',\n",
        "                                      'Home.dest']\n",
        "                   })\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "print(data[0].page_content)\n",
        "\n",
        "data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r7kgrAcAllp",
        "outputId": "ceaed51e-17aa-48ba-aca0-dbcac51782a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pclass: ﻿pclass\n",
            "Survived: survived\n",
            "Name: name\n",
            "Sex: sex\n",
            "Age: age\n",
            "SibSp: sibsp\n",
            "Parch: parch\n",
            "Ticket: ticket\n",
            "Fare: fare\n",
            "Cabin: cabin\n",
            "Embarked: embarked\n",
            "Boat: boat\n",
            "Body: body\n",
            "Home.dest: home.dest\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/titanic3.csv', 'row': 0}, page_content='Pclass: \\ufeffpclass\\nSurvived: survived\\nName: name\\nSex: sex\\nAge: age\\nSibSp: sibsp\\nParch: parch\\nTicket: ticket\\nFare: fare\\nCabin: cabin\\nEmbarked: embarked\\nBoat: boat\\nBody: body\\nHome.dest: home.dest'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 1}, page_content='Pclass: 1\\nSurvived: 1\\nName: Allen, Miss. Elisabeth Walton\\nSex: female\\nAge: 29\\nSibSp: 0\\nParch: 0\\nTicket: 24160\\nFare: 211.3375\\nCabin: B5\\nEmbarked: S\\nBoat: 2\\nBody: \\nHome.dest: St Louis, MO'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 2}, page_content='Pclass: 1\\nSurvived: 1\\nName: Allison, Master. Hudson Trevor\\nSex: male\\nAge: 0.9167\\nSibSp: 1\\nParch: 2\\nTicket: 113781\\nFare: 151.5500\\nCabin: C22 C26\\nEmbarked: S\\nBoat: 11\\nBody: \\nHome.dest: Montreal, PQ / Chesterville, ON'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 3}, page_content='Pclass: 1\\nSurvived: 0\\nName: Allison, Miss. Helen Loraine\\nSex: female\\nAge: 2\\nSibSp: 1\\nParch: 2\\nTicket: 113781\\nFare: 151.5500\\nCabin: C22 C26\\nEmbarked: S\\nBoat: \\nBody: \\nHome.dest: Montreal, PQ / Chesterville, ON'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 4}, page_content='Pclass: 1\\nSurvived: 0\\nName: Allison, Mr. Hudson Joshua Creighton\\nSex: male\\nAge: 30\\nSibSp: 1\\nParch: 2\\nTicket: 113781\\nFare: 151.5500\\nCabin: C22 C26\\nEmbarked: S\\nBoat: \\nBody: 135\\nHome.dest: Montreal, PQ / Chesterville, ON'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 5}, page_content='Pclass: 1\\nSurvived: 0\\nName: Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\\nSex: female\\nAge: 25\\nSibSp: 1\\nParch: 2\\nTicket: 113781\\nFare: 151.5500\\nCabin: C22 C26\\nEmbarked: S\\nBoat: \\nBody: \\nHome.dest: Montreal, PQ / Chesterville, ON'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 6}, page_content='Pclass: 1\\nSurvived: 1\\nName: Anderson, Mr. Harry\\nSex: male\\nAge: 48\\nSibSp: 0\\nParch: 0\\nTicket: 19952\\nFare: 26.5500\\nCabin: E12\\nEmbarked: S\\nBoat: 3\\nBody: \\nHome.dest: New York, NY'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 7}, page_content='Pclass: 1\\nSurvived: 1\\nName: Andrews, Miss. Kornelia Theodosia\\nSex: female\\nAge: 63\\nSibSp: 1\\nParch: 0\\nTicket: 13502\\nFare: 77.9583\\nCabin: D7\\nEmbarked: S\\nBoat: 10\\nBody: \\nHome.dest: Hudson, NY'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 8}, page_content='Pclass: 1\\nSurvived: 0\\nName: Andrews, Mr. Thomas Jr\\nSex: male\\nAge: 39\\nSibSp: 0\\nParch: 0\\nTicket: 112050\\nFare: 0.0000\\nCabin: A36\\nEmbarked: S\\nBoat: \\nBody: \\nHome.dest: Belfast, NI'),\n",
              " Document(metadata={'source': '/content/titanic3.csv', 'row': 9}, page_content='Pclass: 1\\nSurvived: 1\\nName: Appleton, Mrs. Edward Dale (Charlotte Lamson)\\nSex: female\\nAge: 53\\nSibSp: 2\\nParch: 0\\nTicket: 11769\\nFare: 51.4792\\nCabin: C101\\nEmbarked: S\\nBoat: D\\nBody: \\nHome.dest: Bayside, Queens, NY')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}