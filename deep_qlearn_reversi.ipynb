{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongug/Fly_python/blob/main/deep_qlearn_reversi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZfPRt6dvp-vq",
      "metadata": {
        "id": "ZfPRt6dvp-vq"
      },
      "source": [
        "# Required packages & your custum environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hB4l0jK3os_M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB4l0jK3os_M",
        "outputId": "fec354e9-ae39-4dde-f20c-051186c71ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/953.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/953.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t5RLz0TOoy95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5RLz0TOoy95",
        "outputId": "ef5ac5c2-977a-42de-f2a4-4908173d01be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lEhYL1OKpch_",
      "metadata": {
        "id": "lEhYL1OKpch_"
      },
      "outputs": [],
      "source": [
        "!cp -r drive/MyDrive/gym_examples /content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-uu9osLeqjSJ",
      "metadata": {
        "id": "-uu9osLeqjSJ"
      },
      "source": [
        "# Importations & constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be000272-b287-41c7-b5ed-1bd352af1a71",
      "metadata": {
        "id": "be000272-b287-41c7-b5ed-1bd352af1a71",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance\n",
        "                                             # of random action being taken\n",
        "batch_size = 16  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 60\n",
        "max_episodes = 5000\n",
        "\n",
        "num_actions = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FFVVSMNXp8W9",
      "metadata": {
        "id": "FFVVSMNXp8W9"
      },
      "outputs": [],
      "source": [
        "# Experience replay buffers\n",
        "action_history = []\n",
        "action_mask_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "\n",
        "# Variables for counting over episodes\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 1000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 10000.0\n",
        "# Maximum replay length\n",
        "max_memory_length = 500000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yW65jkIvqnKe",
      "metadata": {
        "id": "yW65jkIvqnKe"
      },
      "source": [
        "# Loading gym environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bdb4550-1558-4ba7-a910-9faa34652511",
      "metadata": {
        "id": "5bdb4550-1558-4ba7-a910-9faa34652511",
        "tags": []
      },
      "outputs": [],
      "source": [
        "env = gym.make('gym_examples:gym_examples/Reversi-v0', render_mode=\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68262c3f",
      "metadata": {
        "id": "68262c3f"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224573a5",
      "metadata": {
        "id": "224573a5"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the state\n",
        "def preprocess_state(env_observ):\n",
        "    st = torch.from_numpy(env_observ).squeeze()\n",
        "    st = st.to(torch.int64)\n",
        "    st = torch.nn.functional.one_hot(st,num_classes=3)\n",
        "    st = st.permute(2, 0, 1)\n",
        "    return st.to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "plBCTNfiqt3n",
      "metadata": {
        "id": "plBCTNfiqt3n"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85ef96c-cc48-426a-b2f3-12e002502bc9",
      "metadata": {
        "id": "f85ef96c-cc48-426a-b2f3-12e002502bc9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class QModel(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super(QModel, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding='same')\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding='same')\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(1152, 512)\n",
        "        self.fc2 = nn.Linear(512, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        action = self.fc2(x)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f",
      "metadata": {
        "id": "8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "# make a action.\n",
        "model = QModel(num_actions)\n",
        "model.to(device)\n",
        "\n",
        "# Build a target model for the prediction of future rewards.\n",
        "# The weights of a target model get updated every 10000 steps thus when the\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\n",
        "model_target = QModel(num_actions)\n",
        "model_target.to(device)\n",
        "\n",
        "loss_function = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip74_Tt4VW-O",
        "outputId": "4c6c805e-63b8-40b7-cc40-8cdaca1476e8"
      },
      "id": "Ip74_Tt4VW-O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MX7sCGp-sRVa",
      "metadata": {
        "id": "MX7sCGp-sRVa"
      },
      "source": [
        "# Policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1866419c-5bac-484e-b527-94b07c0087e4",
      "metadata": {
        "id": "1866419c-5bac-484e-b527-94b07c0087e4"
      },
      "outputs": [],
      "source": [
        "# Function to select an action\n",
        "# model: the torch model to compuate action-state value (i.e., q-value)\n",
        "# state: a torch tensor (3 x 8 x 8) of float32, which is output by preprocess_state\n",
        "# mask: a 64-size array (np.array)\n",
        "def get_greedy_epsilon(model, state, mask):\n",
        "    global epsilon\n",
        "\n",
        "    #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n",
        "    if np.random.rand(1)[0] < epsilon:\n",
        "        action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            # add a batch axis\n",
        "            state_tensor = state.unsqueeze(0)\n",
        "            # compute the q-values\n",
        "            q_values = model(state_tensor)\n",
        "            # select the q-values of valid actions\n",
        "            action = torch.argmax(\n",
        "                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n",
        "                dim=0)\n",
        "\n",
        "    # decay epsilon\n",
        "    epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "    epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e67c561-ffd1-4e28-977e-c285430e4d9c",
      "metadata": {
        "id": "2e67c561-ffd1-4e28-977e-c285430e4d9c"
      },
      "outputs": [],
      "source": [
        "def get_greedy_action(model, state, mask):\n",
        "    global epsilon\n",
        "\n",
        "    with torch.no_grad():\n",
        "        state_tensor = state.unsqueeze(0) # batch dimension\n",
        "        q_values = model(state_tensor)\n",
        "\n",
        "        action = torch.argmax(\n",
        "                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n",
        "                dim=0)\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3cbad6",
      "metadata": {
        "id": "df3cbad6"
      },
      "source": [
        "# Replay Buffer Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cece0b-2d15-4061-afc5-e94b8cabf7a0",
      "metadata": {
        "id": "94cece0b-2d15-4061-afc5-e94b8cabf7a0"
      },
      "outputs": [],
      "source": [
        "# sample a batch of _batch_size from replay buffers\n",
        "# return numpy.ndarrays\n",
        "def sample_batch(_batch_size):\n",
        "    # Get indices of samples for replay buffers\n",
        "    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n",
        "\n",
        "    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n",
        "    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n",
        "    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
        "    action_sample = np.array([action_history[i] for i in indices])\n",
        "\n",
        "    # action mask is the mask for the valid actions at the '''next''' state\n",
        "    action_mask_sample = np.array([action_mask_history[i] for i in indices])\n",
        "    done_sample = np.array([float(done_history[i]) for i in indices])\n",
        "\n",
        "    return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zDoOM1rFx7if",
      "metadata": {
        "id": "zDoOM1rFx7if"
      },
      "outputs": [],
      "source": [
        "def append_history(state, state_next, reward, action, action_mask, done):\n",
        "    # Save actions and states in replay buffer\n",
        "    action_history.append(action)\n",
        "    action_mask_history.append(action_mask)\n",
        "    state_history.append(state)\n",
        "    state_next_history.append(state_next)\n",
        "    rewards_history.append(reward)\n",
        "    done_history.append(done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3829c226",
      "metadata": {
        "id": "3829c226"
      },
      "outputs": [],
      "source": [
        "# Function to update the Q-network\n",
        "def update_network():\n",
        "    # sample a batch of ...\n",
        "    state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n",
        "        sample_batch(batch_size)\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    state_sample = torch.tensor(state_sample, dtype=torch.float32).to(device)\n",
        "    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32).to(device)\n",
        "    action_sample = torch.tensor(action_sample, dtype=torch.int64).to(device)\n",
        "    action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64).to(device)\n",
        "    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32).to(device)\n",
        "    done_sample = torch.tensor(done_sample, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Compute the target Q-values for the states\n",
        "    with torch.no_grad():\n",
        "        future_rewards = model_target(state_next_sample)\n",
        "        #future_rewards = future_rewards.cpu()\n",
        "\n",
        "        # compute the q-value for the next state and the action maximizing the q-value\n",
        "        # note: the action should be valid (i.e., mask is set to 1)\n",
        "        max_q_values = torch.max(\n",
        "            future_rewards + action_mask_sample * 100., # trick to select a valid action\n",
        "            dim=1).values.detach() - 100.\n",
        "\n",
        "        # compute the target q-value\n",
        "        # if the step was final, max_q_values should not be added\n",
        "        # we assume that the negative return of the opposite player is the return of next step\n",
        "        # that is, G(t) = r(t+1) - g*r(t+2) + g^2*r(t+3) - g^3*r(t+4) + ...\n",
        "        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n",
        "\n",
        "    # It's forward propagation! Compute the Q-values for the taken actions\n",
        "    q_values = model(state_sample)\n",
        "    #q_values = q_values.cpu()\n",
        "    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_function(q_values_action, target_q_values)\n",
        "\n",
        "    # Perform the optimization step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ed4dcc-f24e-4990-879a-c2f6af51a063",
      "metadata": {
        "id": "90ed4dcc-f24e-4990-879a-c2f6af51a063"
      },
      "source": [
        "# Run DQN Tranining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kae3oT2ozMzK",
      "metadata": {
        "id": "Kae3oT2ozMzK"
      },
      "outputs": [],
      "source": [
        "# Experience replay buffers\n",
        "action_history = []\n",
        "action_mask_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "343adc5c-37d7-4429-890e-b720a291548c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "343adc5c-37d7-4429-890e-b720a291548c",
        "outputId": "7fe54c02-3871-4f4e-b259-78fafdfd25cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be uint8, actual type: int16\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be uint8, actual type: int16\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 10, Frame count: 299, Running reward: -73.0\n",
            "Episode: 20, Frame count: 597, Running reward: -50.55\n",
            "Episode: 30, Frame count: 895, Running reward: -57.766666666666666\n",
            "Episode: 40, Frame count: 1195, Running reward: -64.525\n",
            "Episode: 50, Frame count: 1493, Running reward: -64.18\n",
            "Episode: 60, Frame count: 1791, Running reward: -64.05\n",
            "Episode: 70, Frame count: 2068, Running reward: -59.4\n",
            "Episode: 80, Frame count: 2367, Running reward: -60.9875\n",
            "Episode: 90, Frame count: 2667, Running reward: -59.766666666666666\n",
            "Episode: 100, Frame count: 2964, Running reward: -55.37\n",
            "Episode: 110, Frame count: 3263, Running reward: -53.13\n",
            "Episode: 120, Frame count: 3561, Running reward: -56.47\n",
            "Episode: 130, Frame count: 3832, Running reward: -51.17\n",
            "Episode: 140, Frame count: 4132, Running reward: -44.71\n",
            "Episode: 150, Frame count: 4430, Running reward: -41.37\n",
            "Episode: 160, Frame count: 4725, Running reward: -39.98\n",
            "Episode: 170, Frame count: 5025, Running reward: -41.89\n",
            "Episode: 180, Frame count: 5323, Running reward: -41.07\n",
            "Episode: 190, Frame count: 5618, Running reward: -38.75\n",
            "Episode: 200, Frame count: 5916, Running reward: -43.26\n",
            "Episode: 210, Frame count: 6212, Running reward: -42.04\n",
            "Episode: 220, Frame count: 6512, Running reward: -36.55\n",
            "Episode: 230, Frame count: 6805, Running reward: -38.69\n",
            "Episode: 240, Frame count: 7100, Running reward: -42.73\n",
            "Episode: 250, Frame count: 7400, Running reward: -43.58\n",
            "Episode: 260, Frame count: 7699, Running reward: -42.57\n",
            "Episode: 270, Frame count: 7996, Running reward: -42.61\n",
            "Episode: 280, Frame count: 8295, Running reward: -37.71\n",
            "Episode: 290, Frame count: 8590, Running reward: -39.99\n",
            "Episode: 300, Frame count: 8887, Running reward: -35.35\n",
            "Episode: 310, Frame count: 9187, Running reward: -32.03\n",
            "Episode: 320, Frame count: 9482, Running reward: -31.87\n",
            "Episode: 330, Frame count: 9755, Running reward: -31.4\n",
            "Episode: 340, Frame count: 10028, Running reward: -26.99\n",
            "Episode: 350, Frame count: 10321, Running reward: -27.02\n",
            "Episode: 360, Frame count: 10611, Running reward: -25.89\n",
            "Episode: 370, Frame count: 10911, Running reward: -22.16\n",
            "Episode: 380, Frame count: 11211, Running reward: -25.74\n",
            "Episode: 390, Frame count: 11506, Running reward: -25.55\n",
            "Episode: 400, Frame count: 11805, Running reward: -30.04\n",
            "Episode: 410, Frame count: 12102, Running reward: -33.2\n",
            "Episode: 420, Frame count: 12396, Running reward: -33.41\n",
            "Episode: 430, Frame count: 12696, Running reward: -30.24\n",
            "Episode: 440, Frame count: 12996, Running reward: -30.92\n",
            "Episode: 450, Frame count: 13277, Running reward: -27.4\n",
            "Episode: 460, Frame count: 13577, Running reward: -24.89\n",
            "Episode: 470, Frame count: 13875, Running reward: -28.47\n",
            "Episode: 480, Frame count: 14156, Running reward: -22.71\n",
            "Episode: 490, Frame count: 14449, Running reward: -22.91\n",
            "Episode: 500, Frame count: 14749, Running reward: -21.91\n",
            "Episode: 510, Frame count: 15048, Running reward: -20.87\n",
            "Episode: 520, Frame count: 15346, Running reward: -24.97\n",
            "Episode: 530, Frame count: 15644, Running reward: -27.0\n",
            "Episode: 540, Frame count: 15941, Running reward: -27.28\n",
            "Episode: 550, Frame count: 16236, Running reward: -27.64\n",
            "Episode: 560, Frame count: 16535, Running reward: -29.89\n",
            "Episode: 570, Frame count: 16817, Running reward: -27.57\n",
            "Episode: 580, Frame count: 17116, Running reward: -27.41\n",
            "Episode: 590, Frame count: 17415, Running reward: -26.15\n",
            "Episode: 600, Frame count: 17712, Running reward: -23.63\n",
            "Episode: 610, Frame count: 17987, Running reward: -22.55\n",
            "Episode: 620, Frame count: 18282, Running reward: -20.66\n",
            "Episode: 630, Frame count: 18577, Running reward: -19.64\n",
            "Episode: 640, Frame count: 18873, Running reward: -19.69\n",
            "Episode: 650, Frame count: 19170, Running reward: -23.81\n",
            "Episode: 660, Frame count: 19470, Running reward: -23.96\n",
            "Episode: 670, Frame count: 19743, Running reward: -26.24\n",
            "Episode: 680, Frame count: 20042, Running reward: -29.97\n",
            "Episode: 690, Frame count: 20336, Running reward: -28.65\n",
            "Episode: 700, Frame count: 20634, Running reward: -26.49\n",
            "Episode: 710, Frame count: 20934, Running reward: -27.57\n",
            "Episode: 720, Frame count: 21229, Running reward: -28.16\n",
            "Episode: 730, Frame count: 21525, Running reward: -27.15\n",
            "Episode: 740, Frame count: 21824, Running reward: -29.47\n",
            "Episode: 750, Frame count: 22098, Running reward: -27.24\n",
            "Episode: 760, Frame count: 22397, Running reward: -28.09\n",
            "Episode: 770, Frame count: 22697, Running reward: -25.43\n",
            "Episode: 780, Frame count: 22974, Running reward: -25.12\n",
            "Episode: 790, Frame count: 23271, Running reward: -22.95\n",
            "Episode: 800, Frame count: 23569, Running reward: -25.46\n",
            "Episode: 810, Frame count: 23867, Running reward: -24.36\n",
            "Episode: 820, Frame count: 24164, Running reward: -23.84\n",
            "Episode: 830, Frame count: 24463, Running reward: -26.97\n",
            "Episode: 840, Frame count: 24759, Running reward: -25.73\n",
            "Episode: 850, Frame count: 25050, Running reward: -24.45\n",
            "Episode: 860, Frame count: 25348, Running reward: -24.75\n",
            "Episode: 870, Frame count: 25644, Running reward: -25.0\n",
            "Episode: 880, Frame count: 25943, Running reward: -25.13\n",
            "Episode: 890, Frame count: 26218, Running reward: -26.46\n",
            "Episode: 900, Frame count: 26515, Running reward: -26.91\n",
            "Episode: 910, Frame count: 26790, Running reward: -26.9\n",
            "Episode: 920, Frame count: 27090, Running reward: -28.74\n",
            "Episode: 930, Frame count: 27386, Running reward: -25.68\n",
            "Episode: 940, Frame count: 27686, Running reward: -28.18\n",
            "Episode: 950, Frame count: 27985, Running reward: -28.55\n",
            "Episode: 960, Frame count: 28285, Running reward: -26.15\n",
            "Episode: 970, Frame count: 28578, Running reward: -27.53\n",
            "Episode: 980, Frame count: 28877, Running reward: -28.71\n",
            "Episode: 990, Frame count: 29174, Running reward: -28.54\n",
            "Episode: 1000, Frame count: 29456, Running reward: -28.95\n",
            "Episode: 1010, Frame count: 29752, Running reward: -30.12\n",
            "Episode: 1020, Frame count: 30037, Running reward: -26.52\n",
            "Episode: 1030, Frame count: 30335, Running reward: -30.92\n",
            "Episode: 1040, Frame count: 30613, Running reward: -29.76\n",
            "Episode: 1050, Frame count: 30910, Running reward: -30.86\n",
            "Episode: 1060, Frame count: 31186, Running reward: -33.25\n",
            "Episode: 1070, Frame count: 31484, Running reward: -32.0\n",
            "Episode: 1080, Frame count: 31784, Running reward: -31.02\n",
            "Episode: 1090, Frame count: 32075, Running reward: -30.98\n",
            "Episode: 1100, Frame count: 32375, Running reward: -30.96\n",
            "Episode: 1110, Frame count: 32632, Running reward: -32.35\n",
            "Episode: 1120, Frame count: 32926, Running reward: -31.44\n",
            "Episode: 1130, Frame count: 33225, Running reward: -30.25\n",
            "Episode: 1140, Frame count: 33524, Running reward: -28.65\n",
            "Episode: 1150, Frame count: 33823, Running reward: -27.57\n",
            "Episode: 1160, Frame count: 34122, Running reward: -25.12\n",
            "Episode: 1170, Frame count: 34409, Running reward: -29.65\n",
            "Episode: 1180, Frame count: 34704, Running reward: -28.15\n",
            "Episode: 1190, Frame count: 35002, Running reward: -31.31\n",
            "Episode: 1200, Frame count: 35302, Running reward: -27.79\n",
            "Episode: 1210, Frame count: 35580, Running reward: -23.93\n",
            "Episode: 1220, Frame count: 35861, Running reward: -26.31\n",
            "Episode: 1230, Frame count: 36141, Running reward: -23.1\n",
            "Episode: 1240, Frame count: 36420, Running reward: -22.46\n",
            "Episode: 1250, Frame count: 36716, Running reward: -23.19\n",
            "Episode: 1260, Frame count: 37013, Running reward: -27.79\n",
            "Episode: 1270, Frame count: 37311, Running reward: -25.7\n",
            "Episode: 1280, Frame count: 37594, Running reward: -25.85\n",
            "Episode: 1290, Frame count: 37870, Running reward: -24.06\n",
            "Episode: 1300, Frame count: 38167, Running reward: -26.29\n",
            "Episode: 1310, Frame count: 38467, Running reward: -27.62\n",
            "Episode: 1320, Frame count: 38761, Running reward: -27.59\n",
            "Episode: 1330, Frame count: 39059, Running reward: -27.7\n",
            "Episode: 1340, Frame count: 39357, Running reward: -27.41\n",
            "Episode: 1350, Frame count: 39655, Running reward: -30.03\n",
            "Episode: 1360, Frame count: 39954, Running reward: -26.52\n",
            "Episode: 1370, Frame count: 40254, Running reward: -22.4\n",
            "Episode: 1380, Frame count: 40549, Running reward: -22.06\n",
            "Episode: 1390, Frame count: 40845, Running reward: -19.65\n",
            "Episode: 1400, Frame count: 41144, Running reward: -19.83\n",
            "Episode: 1410, Frame count: 41432, Running reward: -22.28\n",
            "Episode: 1420, Frame count: 41729, Running reward: -20.83\n",
            "Episode: 1430, Frame count: 42026, Running reward: -19.08\n",
            "Episode: 1440, Frame count: 42325, Running reward: -18.89\n",
            "Episode: 1450, Frame count: 42620, Running reward: -16.42\n",
            "Episode: 1460, Frame count: 42919, Running reward: -16.61\n",
            "Episode: 1470, Frame count: 43219, Running reward: -19.28\n",
            "Episode: 1480, Frame count: 43518, Running reward: -19.54\n",
            "Episode: 1490, Frame count: 43815, Running reward: -21.7\n",
            "Episode: 1500, Frame count: 44115, Running reward: -22.62\n",
            "Episode: 1510, Frame count: 44388, Running reward: -19.9\n",
            "Episode: 1520, Frame count: 44649, Running reward: -24.45\n",
            "Episode: 1530, Frame count: 44946, Running reward: -27.25\n",
            "Episode: 1540, Frame count: 45244, Running reward: -28.36\n",
            "Episode: 1550, Frame count: 45541, Running reward: -31.72\n",
            "Episode: 1560, Frame count: 45835, Running reward: -29.5\n",
            "Episode: 1570, Frame count: 46092, Running reward: -28.7\n",
            "Episode: 1580, Frame count: 46386, Running reward: -29.82\n",
            "Episode: 1590, Frame count: 46683, Running reward: -29.98\n",
            "Episode: 1600, Frame count: 46982, Running reward: -26.63\n",
            "Episode: 1610, Frame count: 47277, Running reward: -30.04\n",
            "Episode: 1620, Frame count: 47573, Running reward: -25.73\n",
            "Episode: 1630, Frame count: 47872, Running reward: -26.57\n",
            "Episode: 1640, Frame count: 48172, Running reward: -25.83\n",
            "Episode: 1650, Frame count: 48470, Running reward: -21.22\n",
            "Episode: 1660, Frame count: 48769, Running reward: -24.39\n",
            "Episode: 1670, Frame count: 49065, Running reward: -26.28\n",
            "Episode: 1680, Frame count: 49355, Running reward: -27.67\n",
            "Episode: 1690, Frame count: 49651, Running reward: -25.44\n",
            "Episode: 1700, Frame count: 49946, Running reward: -28.24\n",
            "Episode: 1710, Frame count: 50224, Running reward: -26.08\n",
            "Episode: 1720, Frame count: 50521, Running reward: -28.18\n",
            "Episode: 1730, Frame count: 50783, Running reward: -30.68\n",
            "Episode: 1740, Frame count: 51073, Running reward: -29.39\n",
            "Episode: 1750, Frame count: 51371, Running reward: -29.42\n",
            "Episode: 1760, Frame count: 51668, Running reward: -26.19\n",
            "Episode: 1770, Frame count: 51960, Running reward: -20.54\n",
            "Episode: 1780, Frame count: 52256, Running reward: -16.9\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9cf999bf4baa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Take the selected action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mstate_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mstate_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0maction_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gym_examples/envs/reversi_random.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAGENT_PLAYER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# -------------- fill here --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for _ in range(max_episodes):\n",
        "    state, info = env.reset()\n",
        "    state = preprocess_state(state)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        frame_count += 1\n",
        "\n",
        "        # Select an action\n",
        "        #state_cuda = state.to(device)\n",
        "        action = get_greedy_epsilon(model,\n",
        "                      state.to(device),\n",
        "                      action_mask)\n",
        "        if action < 0:\n",
        "            print(action_mask)\n",
        "\n",
        "        # Take the selected action\n",
        "        state_next, reward, done, _, info = env.step((action // 8, action % 8))\n",
        "        state_next = preprocess_state(state_next)\n",
        "        action_mask = info['action_mask'].reshape((-1,))\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Store the transition in the replay buffer\n",
        "        append_history(state, state_next, reward, action, action_mask, done)\n",
        "\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "            update_network()\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del action_mask_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    episode_count += 1\n",
        "    episode_reward_history.append(episode_reward)\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    if episode_count % 10 == 0:\n",
        "        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n",
        "\n",
        "    if episode_count % 5000 == 0:\n",
        "        torch.save(model, 'model.{}'.format(episode_count))\n",
        "    #if running_reward > 20:\n",
        "    #    print(f\"Solved at episode {episode_count}!\")\n",
        "    #    break\n",
        "\n",
        "\n",
        "torch.save(model, 'model.final')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROYwJvbZamcK",
        "outputId": "38530d21-9a34-4a44-c70a-6f04a478f00c"
      },
      "id": "ROYwJvbZamcK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state of the board:\n",
            "    0 1 2 3 4 5 6 7\n",
            "-------------------\n",
            "0 | 2 2 2 2 2 2 2 2\n",
            "1 | 2 2 1 1 1 1 2 2\n",
            "2 | 2 1 2 2 2 1 2 2\n",
            "3 | 2 1 2 2 2 2 2 2\n",
            "4 | 2 2 1 1 1 2 2 2\n",
            "5 | 2 2 2 2 2 2 2 2\n",
            "6 | 2 2 2 2 1 1 1 2\n",
            "7 | 1 . 1 1 1 1 . 1\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aad31b0-c081-4092-9414-484c98b70370",
      "metadata": {
        "id": "4aad31b0-c081-4092-9414-484c98b70370"
      },
      "outputs": [],
      "source": [
        "torch.save(model.cpu().state_dict(), 'model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgYA2pHAoeI2",
        "outputId": "79b17ae5-39ea-4327-cc96-92c3a68af21d"
      },
      "id": "JgYA2pHAoeI2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  gym_examples  model  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp model drive/MyDrive/SKT"
      ],
      "metadata": {
        "id": "k112vUJmoolQ"
      },
      "id": "k112vUJmoolQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4984b880-e427-48cb-bf91-13a91d6529f5",
      "metadata": {
        "id": "4984b880-e427-48cb-bf91-13a91d6529f5"
      },
      "source": [
        "# Evaluation (Agent vs. Gym's random play)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c198b4e-b0e4-4fd4-821d-be602c2dbc5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "5c198b4e-b0e4-4fd4-821d-be602c2dbc5a",
        "outputId": "26670483-9747-4cdd-e69a-38dd4df64265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state of the board:\n",
            "    0 1 2 3 4 5 6 7\n",
            "-------------------\n",
            "0 | . . . . . . . .\n",
            "1 | . . . . . . . .\n",
            "2 | . . . . . . . .\n",
            "3 | . . . 1 2 . . .\n",
            "4 | . . . 2 1 . . .\n",
            "5 | . . . . . . . .\n",
            "6 | . . . . . . . .\n",
            "7 | . . . . . . . .\n",
            "<class 'str'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8ba105ed77e2>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"action: ({}, {})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-f93b6dc760df>\u001b[0m in \u001b[0;36mget_greedy_action\u001b[0;34m(model, state, mask)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         action = torch.argmax(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-fb2094ee3df2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ],
      "source": [
        "import time, sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "board, info = env.reset()\n",
        "state = preprocess_state(board)\n",
        "action_mask = info['action_mask'].reshape((-1,))\n",
        "done = False\n",
        "env.render()\n",
        "\n",
        "while not done:\n",
        "    action = get_greedy_action(model, state.to(device), action_mask)\n",
        "    print(\"action: ({}, {})\".format(action // 8, action % 8))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    time.sleep(1.0)\n",
        "    clear_output(wait=False)\n",
        "    board, reward, done, _, info = env.step((action // 8, action % 8))\n",
        "    state = preprocess_state(board)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation (Agent vs. Human)"
      ],
      "metadata": {
        "id": "cgKRLo18cnhe"
      },
      "id": "cgKRLo18cnhe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06aaaf66-05b5-4f6a-a004-5016908f8df4",
      "metadata": {
        "id": "06aaaf66-05b5-4f6a-a004-5016908f8df4"
      },
      "outputs": [],
      "source": [
        "board, info = env.reset()\n",
        "state = preprocess_state(board)\n",
        "action_mask = info['action_mask'].reshape((-1,))\n",
        "done = False\n",
        "env.render()\n",
        "\n",
        "while not done:\n",
        "    action = get_greedy_action(model, state.to(device), action_mask)\n",
        "    print(\"action: ({}, {})\".format(action // 8, action % 8))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    time.sleep(1.0)\n",
        "    clear_output(wait=False)\n",
        "    board, reward, done, _, info = env.step((action // 8, action % 8))\n",
        "    state = preprocess_state(board)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "    env.render()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}